{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TQEa8erKI_k",
        "outputId": "eeccb5ae-86e6-42e9-cc42-0162de71e6fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[INFO] Using device: cuda\n",
            "[INFO] Cleared GPU memory\n",
            "[INFO] Class weights: [0.99983333 0.99983333 0.99983333 1.0015025  0.99983333 0.99983333\n",
            " 0.99983333 0.99983333 0.99983333 0.99983333]\n",
            "Train samples: 4199, Val samples: 899, Test samples: 901\n",
            "[INFO] Training started at 2025-09-12 13:49:45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 525/525 [44:27<00:00,  5.08s/it, acc=56.8, loss=0.589]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Checkpoint saved at epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Val]: 100%|██████████| 113/113 [09:10<00:00,  4.87s/it, acc=78.4, loss=0.054]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss 1.1400, Val Loss 0.5687, Train Acc 56.82%, Val Acc 78.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 525/525 [00:38<00:00, 13.81it/s, acc=87.4, loss=0.265]\n",
            "Epoch 2/10 [Val]: 100%|██████████| 113/113 [00:04<00:00, 26.29it/s, acc=94.1, loss=0.0145]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss 0.3720, Val Loss 0.2007, Train Acc 87.38%, Val Acc 94.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 525/525 [00:37<00:00, 13.82it/s, acc=92.6, loss=0.0986]\n",
            "Epoch 3/10 [Val]: 100%|██████████| 113/113 [00:04<00:00, 24.96it/s, acc=88.7, loss=0.0137]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss 0.2428, Val Loss 0.3175, Train Acc 92.62%, Val Acc 88.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 525/525 [00:37<00:00, 14.05it/s, acc=95.4, loss=0.261]\n",
            "Epoch 4/10 [Val]: 100%|██████████| 113/113 [00:04<00:00, 25.04it/s, acc=92.5, loss=0.00485]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss 0.1612, Val Loss 0.2434, Train Acc 95.36%, Val Acc 92.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 525/525 [00:37<00:00, 13.88it/s, acc=95.4, loss=0.00745]\n",
            "Epoch 5/10 [Val]: 100%|██████████| 113/113 [00:04<00:00, 26.62it/s, acc=97, loss=0.0112]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss 0.1506, Val Loss 0.1174, Train Acc 95.38%, Val Acc 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 525/525 [00:38<00:00, 13.76it/s, acc=95.9, loss=0.0338]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Checkpoint saved at epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 [Val]: 100%|██████████| 113/113 [00:05<00:00, 21.92it/s, acc=95.9, loss=0.00449]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Loss 0.1322, Val Loss 0.1519, Train Acc 95.93%, Val Acc 95.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 525/525 [00:38<00:00, 13.70it/s, acc=97.3, loss=0.011]\n",
            "Epoch 7/10 [Val]: 100%|██████████| 113/113 [00:04<00:00, 24.31it/s, acc=97.4, loss=0.00407]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss 0.0920, Val Loss 0.0990, Train Acc 97.31%, Val Acc 97.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 525/525 [00:37<00:00, 13.89it/s, acc=98, loss=0.0119]\n",
            "Epoch 8/10 [Val]: 100%|██████████| 113/113 [00:04<00:00, 26.54it/s, acc=96.2, loss=0.00315]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss 0.0688, Val Loss 0.1392, Train Acc 97.98%, Val Acc 96.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 525/525 [00:37<00:00, 13.92it/s, acc=97.3, loss=0.0035]\n",
            "Epoch 9/10 [Val]: 100%|██████████| 113/113 [00:04<00:00, 26.52it/s, acc=97.4, loss=0.00262]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Loss 0.0925, Val Loss 0.1144, Train Acc 97.29%, Val Acc 97.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 525/525 [00:37<00:00, 13.93it/s, acc=97.4, loss=0.0412]\n",
            "Epoch 10/10 [Val]: 100%|██████████| 113/113 [00:04<00:00, 26.38it/s, acc=96.3, loss=0.0278]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss 0.0906, Val Loss 0.1426, Train Acc 97.40%, Val Acc 96.33%\n",
            "[INFO] Training curves saved to /content/drive/MyDrive/_Chakma_Numbers_Results_ResNet50_/training_curves.png\n",
            "[INFO] XAI visualizations saved to /content/drive/MyDrive/_Chakma_Numbers_Results_ResNet50_/xai_visualizations.png\n",
            "[RESULT] Test Accuracy: 0.9401\n",
            "[INFO] Confusion matrix saved to /content/drive/MyDrive/_Chakma_Numbers_Results_ResNet50_/confusion_matrix.png\n",
            "[INFO] Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       eight     0.9651    0.9881    0.9765        84\n",
            "        five     0.9118    0.8611    0.8857       108\n",
            "        four     1.0000    0.9059    0.9506        85\n",
            "        nine     0.8796    1.0000    0.9360        95\n",
            "         one     0.9659    0.8854    0.9239        96\n",
            "       seven     0.9072    0.9778    0.9412        90\n",
            "         six     0.9062    0.9560    0.9305        91\n",
            "       three     0.9625    0.9872    0.9747        78\n",
            "         two     0.9870    0.9500    0.9682        80\n",
            "        zero     0.9556    0.9149    0.9348        94\n",
            "\n",
            "    accuracy                         0.9401       901\n",
            "   macro avg     0.9441    0.9426    0.9422       901\n",
            "weighted avg     0.9421    0.9401    0.9399       901\n",
            "\n",
            "[INFO] Classification report saved to /content/drive/MyDrive/_Chakma_Numbers_Results_ResNet50_/classification_report.txt and /content/drive/MyDrive/_Chakma_Numbers_Results_ResNet50_/classification_report.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating ROC data: 100%|██████████| 113/113 [00:04<00:00, 24.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] ROC curve saved to /content/drive/MyDrive/_Chakma_Numbers_Results_ResNet50_/roc_curve.png\n",
            "\n",
            "[INFO] AUC values for each class:\n",
            "  - eight (AUC = 1.00)\n",
            "  - five (AUC = 0.99)\n",
            "  - four (AUC = 1.00)\n",
            "  - nine (AUC = 1.00)\n",
            "  - one (AUC = 1.00)\n",
            "  - seven (AUC = 1.00)\n",
            "  - six (AUC = 1.00)\n",
            "  - three (AUC = 1.00)\n",
            "  - two (AUC = 1.00)\n",
            "  - zero (AUC = 0.99)\n",
            "[INFO] Model saved to /content/drive/MyDrive/_Chakma_Numbers_Results_ResNet50_/chakma_numeral_classifier.pth\n"
          ]
        }
      ],
      "source": [
        "# Chakma Numerals Recognition System\n",
        "# Hybrid CNN-Transformer with Encoder-Decoder Architecture and Explainable AI\n",
        "# With ResNet50 backbone\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import math\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from einops import rearrange, reduce\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[INFO] Using device: {device}\")\n",
        "\n",
        "# Google Drive results directory\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/_Chakma_Numbers_Results_ResNet50_\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Positional Encoding for Transformer\n",
        "# ---------------------------------------------------------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len, :]\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# CBAM Attention\n",
        "# ---------------------------------------------------------------------------\n",
        "class CBAMAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction_ratio=16):\n",
        "        super().__init__()\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(channels, channels // reduction_ratio, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels // reduction_ratio, channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.spatial_attention = nn.Sequential(\n",
        "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Channel attention\n",
        "        channel_att = self.channel_attention(x)\n",
        "        x_channel = x * channel_att\n",
        "\n",
        "        # Spatial attention\n",
        "        avg_out = torch.mean(x_channel, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x_channel, dim=1, keepdim=True)\n",
        "        spatial = torch.cat([avg_out, max_out], dim=1)\n",
        "        spatial_att = self.spatial_attention(spatial)\n",
        "\n",
        "        return x_channel * spatial_att, spatial_att\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Enhanced Hybrid CNN-Transformer Model with Encoder-Decoder\n",
        "# ---------------------------------------------------------------------------\n",
        "class ChakmaNumeralClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, d_model=256, nhead=8, num_encoder_layers=3,\n",
        "                 num_decoder_layers=3, max_seq_length=10, mode='classification'):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_classes = num_classes\n",
        "        self.attention_weights = []  # Store attention weights for visualization\n",
        "\n",
        "        # CNN Backbone (ResNet50) - Updated from ResNet18\n",
        "        backbone = models.resnet50(pretrained=False)\n",
        "        backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.feature_extractor = nn.Sequential(*list(backbone.children())[:-2])\n",
        "\n",
        "        # CBAM Attention - Updated channel size for ResNet50 (2048 channels)\n",
        "        self.cbam = CBAMAttention(2048)\n",
        "\n",
        "        # Adaptive pooling\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Encoder Projection - Updated input size for ResNet50 (2048 channels)\n",
        "        self.encoder_proj = nn.Linear(2048, d_model)\n",
        "        self.encoder_pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=512, dropout=0.1, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "\n",
        "        # Register hooks to capture attention weights\n",
        "        for layer in self.transformer_encoder.layers:\n",
        "            # Check if the submodule is MultiheadAttention before registering\n",
        "            if hasattr(layer, 'self_attn') and isinstance(layer.self_attn, nn.MultiheadAttention):\n",
        "                 layer.self_attn.register_forward_hook(self._get_attention_hook())\n",
        "\n",
        "        # Mode-specific components\n",
        "        if mode == 'classification':\n",
        "            # Classification Head\n",
        "            self.fc = nn.Linear(d_model, num_classes)\n",
        "        elif mode == 'sequence':\n",
        "            # Decoder components for sequence recognition\n",
        "            self.tgt_embedding = nn.Embedding(num_classes + 1, d_model)  # +1 for SOS/EOS tokens\n",
        "            self.decoder_pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "            decoder_layer = nn.TransformerDecoderLayer(\n",
        "                d_model=d_model, nhead=nhead, dim_feedforward=512, dropout=0.1, batch_first=True\n",
        "            )\n",
        "            self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
        "            self.fc_out = nn.Linear(d_model, num_classes + 1)  # +1 for EOS token\n",
        "\n",
        "            # optional CTC (left defined but not used here for simplicity)\n",
        "            self.ctc_loss = nn.CTCLoss(blank=num_classes)\n",
        "\n",
        "    def _get_attention_hook(self):\n",
        "        def hook(module, input, output):\n",
        "            # Store attention weights for visualization\n",
        "            # Check if output is a tuple with at least two elements and the second element is a tensor\n",
        "            if isinstance(output, tuple) and len(output) > 1 and isinstance(output[1], torch.Tensor):\n",
        "                self.attention_weights.append(output[1].detach().cpu())\n",
        "            else:\n",
        "                # Append None if attention weights are not available or not in expected format\n",
        "                self.attention_weights.append(None)\n",
        "        return hook\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        self.attention_weights = []  # Reset attention weights\n",
        "\n",
        "        # Feature extraction (shared)\n",
        "        features = self.feature_extractor(x)\n",
        "        features, spatial_att = self.cbam(features)  # Get spatial attention from CBAM\n",
        "        features_pooled = self.adaptive_pool(features)\n",
        "        features_pooled = features_pooled.view(features_pooled.size(0), -1)  # [B, 2048]\n",
        "\n",
        "        # Project to transformer dimension and add positional enc\n",
        "        encoded = self.encoder_proj(features_pooled).unsqueeze(1)  # [B, 1, d_model]\n",
        "        encoded = self.encoder_pos_encoding(encoded)\n",
        "\n",
        "        # Memory via transformer encoder\n",
        "        memory = self.transformer_encoder(encoded)  # [B, seq_len=1, d_model]\n",
        "\n",
        "        if self.mode == 'classification':\n",
        "            output = memory.squeeze(1)  # [B, d_model]\n",
        "            return self.fc(output), spatial_att  # Return both output and spatial attention\n",
        "\n",
        "        elif self.mode == 'sequence':\n",
        "            batch_size = x.size(0)\n",
        "\n",
        "            if targets is not None:\n",
        "                sos_tokens = torch.zeros(batch_size, 1, dtype=torch.long, device=x.device)\n",
        "                decoder_input = torch.cat([sos_tokens, targets], dim=1)\n",
        "                tgt_embed = self.tgt_embedding(decoder_input)\n",
        "                tgt_embed = self.decoder_pos_encoding(tgt_embed)\n",
        "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(decoder_input.size(1)).to(x.device)\n",
        "                output = self.transformer_decoder(tgt_embed, memory, tgt_mask=tgt_mask)\n",
        "                return self.fc_out(output), spatial_att\n",
        "            else:\n",
        "                # Greedy decode\n",
        "                decoded = torch.zeros(batch_size, self.max_seq_length, dtype=torch.long, device=x.device)\n",
        "                output_tokens = torch.zeros(batch_size, 1, dtype=torch.long, device=x.device)\n",
        "\n",
        "                for t in range(self.max_seq_length):\n",
        "                    tgt_embed = self.tgt_embedding(output_tokens)\n",
        "                    tgt_embed = self.decoder_pos_encoding(tgt_embed)\n",
        "                    tgt_mask = nn.Transformer.generate_square_subsequent_mask(output_tokens.size(1)).to(x.device)\n",
        "                    output = self.transformer_decoder(tgt_embed, memory, tgt_mask=tgt_mask)\n",
        "                    output = self.fc_out(output[:, -1, :])\n",
        "                    next_token = output.argmax(-1)\n",
        "                    decoded[:, t] = next_token\n",
        "                    output_tokens = torch.cat([output_tokens, next_token.unsqueeze(1)], dim=1)\n",
        "\n",
        "                return decoded, spatial_att\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Explainable AI Tools\n",
        "# ---------------------------------------------------------------------------\n",
        "class XAITools:\n",
        "    @staticmethod\n",
        "    def grad_cam(model, input_tensor, target_layer, target_class=None):\n",
        "        \"\"\"Simplified Grad-CAM implementation using input gradients as an approximation.\"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        # Ensure input tensor requires gradients\n",
        "        input_batch = input_tensor.unsqueeze(0).to(device)\n",
        "        input_batch.requires_grad = True\n",
        "\n",
        "        output, _ = model(input_batch)\n",
        "\n",
        "        if target_class is None:\n",
        "            target_class = output.argmax(dim=1).item()\n",
        "\n",
        "        # Backward pass to get gradients w.r.t. input\n",
        "        model.zero_grad()\n",
        "        output[0, target_class].backward()\n",
        "\n",
        "        # Use input gradients as an approximate importance map\n",
        "        if input_batch.grad is not None:\n",
        "            # Average gradients across channels\n",
        "            cam = torch.mean(torch.abs(input_batch.grad[0]), dim=0)\n",
        "            # Resize CAM to original image size\n",
        "            cam = F.interpolate(cam.unsqueeze(0).unsqueeze(0),\n",
        "                                size=input_tensor.shape[1:],\n",
        "                                mode='bilinear',\n",
        "                                align_corners=False).squeeze()\n",
        "            # Normalize CAM\n",
        "            cam = cam - cam.min()\n",
        "            cam = cam / (cam.max() + 1e-8) # Add small epsilon to avoid division by zero\n",
        "            return cam.cpu().numpy(), target_class\n",
        "\n",
        "        return None, target_class\n",
        "\n",
        "    @staticmethod\n",
        "    def visualize_attention(model, input_tensor):\n",
        "        \"\"\"Visualize attention maps from CBAM and transformer.\"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Get spatial attention from CBAM\n",
        "            _, spatial_att = model(input_tensor.unsqueeze(0).to(device))\n",
        "\n",
        "            if spatial_att is not None:\n",
        "                # Process spatial attention\n",
        "                spatial_att = spatial_att.squeeze().cpu().numpy()\n",
        "\n",
        "                # Resize to match input image size\n",
        "                h, w = input_tensor.shape[1], input_tensor.shape[2]\n",
        "                att_mask = cv2.resize(spatial_att, (w, h))\n",
        "\n",
        "                # Normalize\n",
        "                att_mask = (att_mask - att_mask.min()) / (att_mask.max() - att_mask.min() + 1e-8)\n",
        "\n",
        "                # Create attention map by applying mask to original image\n",
        "                img_np = input_tensor.squeeze().cpu().numpy()\n",
        "                att_map = att_mask * img_np\n",
        "\n",
        "                return att_mask, att_map\n",
        "            else:\n",
        "                return None, None\n",
        "\n",
        "    @staticmethod\n",
        "    def feature_importance_analysis(model, dataloader, class_idx, num_samples=10):\n",
        "        \"\"\"Analyze which input pixels/features are important for class_idx\"\"\"\n",
        "        model.eval()\n",
        "        important_features = []\n",
        "\n",
        "        cnt = 0\n",
        "        for images, labels in dataloader:\n",
        "            if cnt >= num_samples:\n",
        "                break\n",
        "            images = images.to(device)\n",
        "            images.requires_grad = True\n",
        "            outputs, _ = model(images)\n",
        "            if outputs.dim() == 1:\n",
        "                outputs = outputs.unsqueeze(0)\n",
        "            model.zero_grad()\n",
        "            # Sum output logits for the class of interest across batch\n",
        "            target_scores = outputs[:, class_idx].sum()\n",
        "            target_scores.backward(retain_graph=False)\n",
        "\n",
        "            # gradient magnitude averaged across channels\n",
        "            if images.grad is not None:\n",
        "                grad_abs = torch.mean(torch.abs(images.grad), dim=1)  # [B, H, W]\n",
        "                important_features.append(grad_abs.detach().cpu().numpy())\n",
        "            images.requires_grad = False\n",
        "            cnt += images.size(0)\n",
        "\n",
        "        if len(important_features) == 0:\n",
        "            return None\n",
        "\n",
        "        return np.mean(np.concatenate(important_features, axis=0), axis=0)  # [H, W]\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Enhanced Dataset with Sequence Support\n",
        "# ---------------------------------------------------------------------------\n",
        "class EnhancedChakmaDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, mode='classification', seq_length=5):\n",
        "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "        self.mode = mode\n",
        "        self.seq_length = seq_length\n",
        "        self.samples = []\n",
        "        self.sequences = []\n",
        "\n",
        "        for cls in self.classes:\n",
        "            cls_dir = os.path.join(root_dir, cls)\n",
        "            for img in os.listdir(cls_dir):\n",
        "                if img.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\")):\n",
        "                    self.samples.append((os.path.join(cls_dir, img), self.class_to_idx[cls]))\n",
        "\n",
        "        if mode == 'sequence':\n",
        "            self._create_sequences()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def _create_sequences(self):\n",
        "        from collections import defaultdict\n",
        "        file_groups = defaultdict(list)\n",
        "        for path, label in self.samples:\n",
        "            base_name = os.path.basename(path).split('_')[0]\n",
        "            file_groups[base_name].append((path, label))\n",
        "        for base_name, samples in file_groups.items():\n",
        "            if len(samples) >= self.seq_length:\n",
        "                samples.sort(key=lambda x: x[0])\n",
        "                for i in range(0, len(samples) - self.seq_length + 1):\n",
        "                    sequence = samples[i:i + self.seq_length]\n",
        "                    self.sequences.append(sequence)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples) if self.mode == 'classification' else len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == 'classification':\n",
        "            path, label = self.samples[idx]\n",
        "            img = Image.open(path).convert(\"L\")\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label\n",
        "        else:\n",
        "            sequence = self.sequences[idx]\n",
        "            images = []\n",
        "            labels = []\n",
        "            for path, label in sequence:\n",
        "                img = Image.open(path).convert(\"L\")\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                images.append(img)\n",
        "                labels.append(label)\n",
        "            images = torch.stack(images)  # [seq_len, C, H, W]\n",
        "            labels = torch.tensor(labels, dtype=torch.long)\n",
        "            return images, labels\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Training and Evaluation Functions\n",
        "# ---------------------------------------------------------------------------\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20, mode='classification'):\n",
        "    # Add progress monitoring\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    print(f\"[INFO] Training started at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "    model.to(device)\n",
        "\n",
        "    # For storing best and worst models\n",
        "    best_model_state = None\n",
        "    best_val_acc = 0.0\n",
        "    worst_model_state = None\n",
        "    worst_val_acc = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss, correct, total = 0.0, 0, 0\n",
        "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "        for batch in train_bar:\n",
        "            if mode == 'classification':\n",
        "                imgs, labels = batch\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs, _ = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            else:\n",
        "                imgs, labels = batch\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs, _ = model(imgs, labels[:, :-1])\n",
        "                loss = criterion(outputs.view(-1, outputs.size(-1)), labels[:, 1:].contiguous().view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            if mode == 'classification':\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "                train_bar.set_postfix(loss=loss.item(), acc=100.0 * correct / total)\n",
        "            else:\n",
        "                train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_train_loss = train_loss / max(1, len(train_loader))\n",
        "        train_acc = 100.0 * correct / total if mode == 'classification' else 0.0\n",
        "        # Add checkpointing\n",
        "        if epoch % 5 == 0:  # Save every 5 epochs\n",
        "            checkpoint_path = os.path.join(RESULTS_DIR, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_train_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"[INFO] Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
        "            for batch in val_bar:\n",
        "                if mode == 'classification':\n",
        "                    imgs, labels = batch\n",
        "                    imgs, labels = imgs.to(device), labels.to(device)\n",
        "                    outputs, _ = model(imgs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += predicted.eq(labels).sum().item()\n",
        "                    val_bar.set_postfix(loss=loss.item(), acc=100.0 * correct / total)\n",
        "                else:\n",
        "                    imgs, labels = batch\n",
        "                    imgs, labels = imgs.to(device), labels.to(device)\n",
        "                    outputs, _ = model(imgs)\n",
        "                    loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "                    val_loss += loss.item()\n",
        "                    val_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_val_loss = val_loss / max(1, len(val_loader))\n",
        "        val_acc = 100.0 * correct / total if mode == 'classification' else 0.0\n",
        "\n",
        "        # Update best and worst models\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "        if val_acc < worst_val_acc:\n",
        "            worst_val_acc = val_acc\n",
        "            worst_model_state = model.state_dict().copy()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            # scheduler expects a scalar to step on\n",
        "            try:\n",
        "                scheduler.step(avg_val_loss)\n",
        "            except Exception:\n",
        "                scheduler.step()\n",
        "\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        history[\"val_loss\"].append(avg_val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}, \"\n",
        "              f\"Train Acc {train_acc:.2f}%, Val Acc {val_acc:.2f}%\")\n",
        "\n",
        "    return model, history, best_model_state, worst_model_state\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Visualization & XAI\n",
        "# ---------------------------------------------------------------------------\n",
        "def plot_curves(history, mode='classification'):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history[\"train_loss\"], label=\"Train\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"Validation\")\n",
        "    plt.title(\"Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
        "\n",
        "    if mode == 'classification':\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history[\"train_acc\"], label=\"Train\")\n",
        "        plt.plot(history[\"val_acc\"], label=\"Validation\")\n",
        "        plt.title(\"Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.legend()\n",
        "\n",
        "    path = os.path.join(RESULTS_DIR, \"training_curves.png\")\n",
        "    plt.savefig(path, dpi=300); plt.close()\n",
        "    print(f\"[INFO] Training curves saved to {path}\")\n",
        "\n",
        "def plot_roc_curve(model, test_loader, dataset, num_classes):\n",
        "    \"\"\"Generate ROC curves for all classes\"\"\"\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Generating ROC data\"):\n",
        "            images = images.to(device)\n",
        "            outputs, _ = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "            all_labels.append(labels.numpy())\n",
        "\n",
        "    all_probs = np.concatenate(all_probs, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    # Binarize the labels for multiclass ROC\n",
        "    y_test_bin = label_binarize(all_labels, classes=range(num_classes))\n",
        "\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], all_probs[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Plot all ROC curves\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = ['blue', 'red', 'green', 'orange', 'purple',\n",
        "              'brown', 'pink', 'gray', 'olive', 'cyan']\n",
        "\n",
        "    for i, color in zip(range(num_classes), colors):\n",
        "        if i < len(dataset.classes):\n",
        "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                     label='{0} (AUC = {1:0.2f})'.format(dataset.classes[i], roc_auc[i]))\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for ResNet50-based Chakma Numeral Classification')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # Save the plot\n",
        "    path = os.path.join(RESULTS_DIR, \"roc_curve.png\")\n",
        "    plt.savefig(path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"[INFO] ROC curve saved to {path}\")\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "def visualize_xai_results(model, test_loader, dataset, num_examples=3):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    xai_tools = XAITools()\n",
        "\n",
        "    samples = []\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "        if i >= num_examples:\n",
        "            break\n",
        "        # If batch, pick first sample of the batch\n",
        "        samples.append((images[0], labels[0]))\n",
        "\n",
        "    fig, axes = plt.subplots(num_examples, 5, figsize=(25, 5 * num_examples))\n",
        "    if num_examples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i, (image, label) in enumerate(samples):\n",
        "        img_np = image.squeeze().cpu().numpy()\n",
        "        axes[i, 0].imshow(img_np, cmap='gray')\n",
        "        axes[i, 0].set_title(f'Original (True: {dataset.classes[label]})')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Choose a convolutional layer as target_layer for Grad-CAM.\n",
        "        target_layer = None\n",
        "        for module in model.feature_extractor.modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                target_layer = module  # last conv found will be used\n",
        "        if target_layer is None:\n",
        "            axes[i, 1].set_title('Grad-CAM not available')\n",
        "            axes[i, 1].axis('off')\n",
        "        else:\n",
        "            cam, pred_class = xai_tools.grad_cam(model, image, target_layer)\n",
        "            if cam is not None:\n",
        "                axes[i, 1].imshow(img_np, cmap='gray')\n",
        "                axes[i, 1].imshow(cam, cmap='jet', alpha=0.5)\n",
        "                axes[i, 1].set_title(f'Grad-CAM (Pred: {dataset.classes[pred_class]})')\n",
        "            else:\n",
        "                axes[i, 1].set_title('Grad-CAM failed')\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "        # Attention visualization\n",
        "        try:\n",
        "            att_mask, att_map = xai_tools.visualize_attention(model, image)\n",
        "            if att_mask is not None:\n",
        "                axes[i, 2].imshow(att_mask, cmap='viridis')\n",
        "                axes[i, 2].set_title('Attention Mask (CBAM)')\n",
        "                axes[i, 2].axis('off')\n",
        "\n",
        "                axes[i, 3].imshow(att_map, cmap='gray')\n",
        "                axes[i, 3].set_title('Attention Map (CBAM)')\n",
        "                axes[i, 3].axis('off')\n",
        "            else:\n",
        "                axes[i, 2].set_title('CBAM Attention Unavailable')\n",
        "                axes[i, 2].axis('off')\n",
        "                axes[i, 3].set_title('CBAM Attention Unavailable')\n",
        "                axes[i, 3].axis('off')\n",
        "\n",
        "        except Exception as e:\n",
        "             axes[i, 2].set_title(f'CBAM Att Error: {str(e)[:30]}...')\n",
        "             axes[i, 2].axis('off')\n",
        "             axes[i, 3].set_title(f'CBAM Att Error: {str(e)[:30]}...')\n",
        "             axes[i, 3].axis('off')\n",
        "\n",
        "        # Class probabilities bar chart\n",
        "        with torch.no_grad():\n",
        "            image_dev = image.unsqueeze(0).to(device)\n",
        "            out, _ = model(image_dev)\n",
        "            probs = F.softmax(out, dim=1)[0].cpu().numpy()\n",
        "            # Plot top-k probabilities to avoid clutter if many classes\n",
        "            classes = dataset.classes\n",
        "            axes[i, 4].barh(classes, probs)\n",
        "            axes[i, 4].set_title('Class Probabilities')\n",
        "            axes[i, 4].set_xlim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    path = os.path.join(RESULTS_DIR, \"xai_visualizations.png\")\n",
        "    plt.savefig(path, dpi=300); plt.close()\n",
        "    print(f\"[INFO] XAI visualizations saved to {path}\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Main Execution\n",
        "# ---------------------------------------------------------------------------\n",
        "def main():\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    # Add memory management\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"[INFO] Cleared GPU memory\")\n",
        "    else:\n",
        "        print(\"[INFO] Using CPU, no GPU memory to clear\")\n",
        "\n",
        "    MODE = 'classification'  # or 'sequence'\n",
        "    root_dir = \"/content/drive/MyDrive/Chakma Numerals\"\n",
        "\n",
        "    if not os.path.isdir(root_dir):\n",
        "        print(f\"[ERROR] Directory not found: {root_dir}\")\n",
        "        print(\"Please ensure Google Drive is mounted and the directory path is correct.\")\n",
        "        return\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.RandomRotation(5),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "\n",
        "    dataset = EnhancedChakmaDataset(root_dir, transform=val_transform, mode=MODE)\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        print(\"[ERROR] Dataset is empty. Check dataset path and structure.\")\n",
        "        return\n",
        "\n",
        "    # Class weights if classification\n",
        "    class_weights = None\n",
        "    if MODE == 'classification':\n",
        "        labels = [label for _, label in dataset.samples]\n",
        "        classes_unique = np.unique(labels)\n",
        "        class_weights_arr = compute_class_weight('balanced', classes=classes_unique, y=labels)\n",
        "        # Build weight vector aligned with dataset.class ordering\n",
        "        # compute_class_weight returns weights for classes_unique; need to map to full range\n",
        "        full_weights = np.ones(len(dataset.classes), dtype=float)\n",
        "        for i, c in enumerate(classes_unique):\n",
        "            full_weights[c] = class_weights_arr[i]\n",
        "        class_weights = torch.tensor(full_weights, dtype=torch.float).to(device)\n",
        "        print(f\"[INFO] Class weights: {full_weights}\")\n",
        "\n",
        "    # Split dataset\n",
        "    n = len(dataset)\n",
        "    train_size = int(0.7 * n)\n",
        "    val_size = int(0.15 * n)\n",
        "    test_size = n - train_size - val_size\n",
        "    train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "    # apply augmentation to train\n",
        "    train_ds.dataset.transform = train_transform\n",
        "\n",
        "    batch_size = 4 if MODE == 'sequence' else 8\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}, Test samples: {len(test_ds)}\")\n",
        "\n",
        "    model = ChakmaNumeralClassifier(\n",
        "        num_classes=len(dataset.classes),\n",
        "        d_model=64,\n",
        "        num_encoder_layers=1,\n",
        "        num_decoder_layers=1,\n",
        "        mode=MODE\n",
        "    ).to(device)\n",
        "\n",
        "    if MODE == 'classification':\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "    model, history, best_model_state, worst_model_state = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10, mode=MODE\n",
        "    )\n",
        "\n",
        "    # Save training curves\n",
        "    plot_curves(history, mode=MODE)\n",
        "\n",
        "    # XAI visualizations\n",
        "    visualize_xai_results(model, test_loader, dataset)\n",
        "\n",
        "    # Test / Metrics\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    all_probs = []  # For ROC curve\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            if MODE == 'classification':\n",
        "                imgs, labels = batch\n",
        "                imgs = imgs.to(device)\n",
        "                outputs, _ = model(imgs)\n",
        "                probs = F.softmax(outputs, dim=1)\n",
        "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "                all_preds.extend(preds.tolist())\n",
        "                all_labels.extend(labels.numpy().tolist())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "            else:\n",
        "                imgs, labels = batch\n",
        "                imgs = imgs.to(device)\n",
        "                outputs, _ = model(imgs)\n",
        "                for i in range(outputs.size(0)):\n",
        "                    seq_pred = outputs[i].cpu().numpy()\n",
        "                    seq_true = labels[i].numpy()\n",
        "                    all_preds.append(seq_pred)\n",
        "                    all_labels.append(seq_true)\n",
        "\n",
        "    if MODE == 'classification':\n",
        "        acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "        print(f\"[RESULT] Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
        "        plt.title(\"Confusion Matrix\"); plt.ylabel(\"True\"); plt.xlabel(\"Predicted\")\n",
        "        plt.xticks(rotation=45); plt.yticks(rotation=0)\n",
        "        cm_path = os.path.join(RESULTS_DIR, \"confusion_matrix.png\")\n",
        "        plt.savefig(cm_path, dpi=300); plt.close()\n",
        "        print(f\"[INFO] Confusion matrix saved to {cm_path}\")\n",
        "\n",
        "        # Classification report (precision, recall, f1)\n",
        "        report = classification_report(all_labels, all_preds, target_names=dataset.classes, digits=4)\n",
        "        print(\"[INFO] Classification Report:\\n\", report)\n",
        "        report_txt = os.path.join(RESULTS_DIR, \"classification_report.txt\")\n",
        "        with open(report_txt, \"w\") as f:\n",
        "            f.write(report)\n",
        "        # Also save as CSV with precision/recall/f1/support columns\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(all_labels, all_preds, labels=range(len(dataset.classes)))\n",
        "        metrics_df = pd.DataFrame({\n",
        "            \"class\": dataset.classes,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1-score\": f1,\n",
        "            \"support\": support\n",
        "        })\n",
        "        metrics_csv = os.path.join(RESULTS_DIR, \"classification_report.csv\")\n",
        "        metrics_df.to_csv(metrics_csv, index=False)\n",
        "        print(f\"[INFO] Classification report saved to {report_txt} and {metrics_csv}\")\n",
        "\n",
        "        # Generate ROC curves\n",
        "        roc_auc = plot_roc_curve(model, test_loader, dataset, len(dataset.classes))\n",
        "\n",
        "        # Print AUC values\n",
        "        print(\"\\n[INFO] AUC values for each class:\")\n",
        "        for i, class_name in enumerate(dataset.classes):\n",
        "            print(f\"  - {class_name} (AUC = {roc_auc[i]:.2f})\")\n",
        "\n",
        "    # Save model checkpoint\n",
        "    model_path = os.path.join(RESULTS_DIR, \"chakma_numeral_classifier.pth\")\n",
        "    torch.save({\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"class_to_idx\": dataset.class_to_idx,\n",
        "        \"classes\": dataset.classes,\n",
        "        \"mode\": MODE\n",
        "    }, model_path)\n",
        "    print(f\"[INFO] Model saved to {model_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}